# From two videos to a 3D scene

How two views of the same dancer (e.g. **gBR_sBM_c01_d04_mBR0_ch01.mp4** and **gBR_sBM_c02_d04_mBR0_ch01.mp4**) become a 3D scene, and what the repo does vs. what is left to implement.

---

## What is `test_fixtures`?

`test_fixtures` is **input test data**, not output.

| Folder | Purpose |
|--------|--------|
| `test_fixtures/synthetic/` | Two small synthetic MP4s (moving circles) generated by the repo. Used by CI so tests don’t need the network or real videos. |
| `test_fixtures/real/` | Two real AIST++ MP4s (e.g. view0 = c01, view1 = c02). Created by `scripts/prepare_test_videos.py --download-aist`. Used for local tests and for running the “two videos → 3D” pipeline. |

Nothing in `test_fixtures` is the result of 3D reconstruction. It’s the **input** we feed into ingestion and then into the 3D pipeline.

---

## Pipeline: two videos → 3D scene

```
  gBR_sBM_c01_...mp4 (camera 1)     gBR_sBM_c02_...mp4 (camera 2)
              \                              /
               \                            /
                v                          v
            INGESTION (this repo)
                \                            /
                 \                          /
                  v                        v
            cam0/frame_00000.png ...   cam1/frame_00000.png ...
                  \                          /
                   \                        /
                    v                      v
            CALIBRATION (camera matrices P1, P2)
                    \                      /
                     v                    v
            2D POSE (e.g. keypoints in each view)
                     \                    /
                      v                  v
            TRIANGULATION (2D → 3D points)
                              |
                              v
                    3D SCENE (points, skeleton, mesh)
```

1. **Ingestion** (implemented)  
   Two videos → time-synced frames per camera: `cam0/frame_00000.png`, `cam1/frame_00000.png`, etc. Same index = same time.

2. **Calibration** (not in repo)  
   Compute each camera’s projection matrix (intrinsics + extrinsics) so we can turn 2D image points into rays in 3D. With AIST++ you can use their provided camera data.

3. **2D pose** (not in repo)  
   Per frame, per view: detect person keypoints (e.g. shoulders, hips) in pixel coordinates. Typical tools: MediaPipe, OpenPose, MMPose.

4. **Triangulation** (demo only in repo)  
   For each keypoint, you have 2D in view0 and 2D in view1. With calibration, `cv2.triangulatePoints(P1, P2, pts0, pts1)` gives the 3D point. Do this for every keypoint and every frame → 3D skeleton over time = 3D scene.

This repo implements **step 1**. Steps 2–4 are downstream (architecture: “optional, not core product”). The script below runs step 1 and a **minimal step 4** (one triangulated point per frame) with placeholder calibration so you can see the flow.

---

## Run the minimal pipeline (ingestion + triangulation)

From `poc/backend`:

```bash
PYTHONPATH=. python scripts/two_videos_to_3d_demo.py \
  ../data/test_fixtures/real/view0.mp4 \
  ../data/test_fixtures/real/view1.mp4 \
  --out-dir ./out_3d_demo
```

- Reads the two MP4s (or use the raw AIST++ c01/c02 paths).
- Runs ingestion: writes synced frames to `out_3d_demo/<session>/cam0/` and `cam1/`.
- Uses a **placeholder** calibration and the **image center** as a fake “keypoint” in both views, then triangulates one 3D point per frame.
- Writes `out_3d_demo/points3d.txt` (one 3D point per frame) and `out_3d_demo/pointcloud.ply` (same points as a PLY point cloud for viewing in MeshLab, CloudCompare, Blender, etc.).

So you see: **2 videos → synced frames → 3D points**. Real 3D would use real calibration (e.g. AIST++ camera data) and 2D pose (e.g. MediaPipe) instead of placeholders.
